{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c0bee66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7681c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "TensorFlow is set to use GPU.\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available and set TensorFlow to use it\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    print(\"GPUs available:\", physical_devices)\n",
    "    try:\n",
    "        for device in physical_devices:\n",
    "            tf.config.experimental.set_memory_growth(device, True)\n",
    "        print(\"TensorFlow is set to use GPU.\")\n",
    "    except Exception as e:\n",
    "        print(\"Could not set memory growth:\", e)\n",
    "else:\n",
    "    print(\"No GPU found. Running on CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25d119a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be4dbb1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2802 images belonging to 10 classes.\n",
      "Found 1069 images belonging to 10 classes.\n",
      "Found 1083 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "train_path = './dataset/train'\n",
    "valid_path = './dataset/val'\n",
    "test_path = './dataset/test'\n",
    "\n",
    "breeds = [\n",
    "    \"Holstein_cow\", \"Jersey_cow\", \"Angus_cow\", \"Brahman_cow\", \"Hereford_cow\",\n",
    "    \"Simmental_cow\", \"Limousin_cow\", \"Guernsey_cow\", \"Charolais_cow\", \"Ayrshire_cow\"\n",
    "]\n",
    "\n",
    "from tensorflow.keras.applications.mobilenet_v3 import preprocess_input\n",
    "\n",
    "train_batches = ImageDataGenerator(preprocessing_function=preprocess_input)\\\n",
    "                .flow_from_directory(directory=train_path, target_size=(224,224), classes=breeds, batch_size = 10)\n",
    "valid_batches = ImageDataGenerator(preprocessing_function=preprocess_input)\\\n",
    "                .flow_from_directory(directory=valid_path, target_size=(224,224), classes=breeds, batch_size = 10)\n",
    "test_batches = ImageDataGenerator(preprocessing_function=preprocess_input)\\\n",
    "                .flow_from_directory(directory=test_path, target_size=(224,224), classes=breeds, batch_size = 10, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df9254a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert train_batches.n >= 1000\n",
    "assert valid_batches.n >= 300\n",
    "assert test_batches.n >= 150\n",
    "assert train_batches.num_classes == valid_batches.num_classes == test_batches.num_classes == 10\n",
    "assert train_batches.batch_size == valid_batches.batch_size == test_batches.batch_size == 10\n",
    "assert train_batches.class_indices == valid_batches.class_indices == test_batches.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78ee6fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the image batch:  (10, 224, 224, 3)\n",
      "Shape of the label batch:  (10, 10)\n"
     ]
    }
   ],
   "source": [
    "imgs , labels = next(train_batches)\n",
    "print(\"Shape of the image batch: \", imgs.shape)\n",
    "print(\"Shape of the label batch: \", labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "941721c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "def plotImages(images_arr):\n",
    "    fig, axes = plt.subplots(1, 10, figsize = (20, 20))\n",
    "    axes = axes.flatten()\n",
    "    for img, ax in zip(images_arr,axes):\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plotImages(imgs)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6709e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Conv2D(filters=32,kernel_size=(3,3),activation='relu', padding='same',input_shape=(224,224,3)),\n",
    "    MaxPool2D(pool_size=(2,2),strides=2),\n",
    "    Conv2D(filters=64, kernel_size = (3,3), activation = 'relu', padding = 'same'),\n",
    "    MaxPool2D(pool_size=(2,2), strides=2),\n",
    "    Flatten(),\n",
    "    Dense(units=10, activation='softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "468396df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA GeForce GTX 1660, compute capability 7.5\n"
     ]
    }
   ],
   "source": [
    "tf.keras.mixed_precision.set_global_policy('mixed_float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8437e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tensorflow.keras.applications import ResNet50, DenseNet121, MobileNetV3Large\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44b355ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(base_model_class, input_shape, num_classes):\n",
    "    base_model = base_model_class(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    x = GlobalAveragePooling2D()(base_model.output)\n",
    "    output = Dense(num_classes, activation='softmax')(x)\n",
    "    model = Model(inputs=base_model.input, outputs=output)\n",
    "    optimizer = Adam(learning_rate=0.001)  \n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "918ecf7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to calculate mAP\n",
    "def mean_average_precision(y_true, y_pred):\n",
    "    # y_true and y_pred should be numpy arrays\n",
    "    # This is a simple implementation for multiclass, multi-label\n",
    "    import sklearn.metrics\n",
    "    return sklearn.metrics.average_precision_score(y_true, y_pred, average='macro')\n",
    "\n",
    "input_shape = (224, 224, 3)\n",
    "num_classes = len(breeds)\n",
    "epochs = 50\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c6659e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MobileNetV3Large...\n",
      "Epoch 1/50\n",
      "281/281 - 48s - loss: 1.3077 - accuracy: 0.5807 - val_loss: 1.4628 - val_accuracy: 0.5800 - 48s/epoch - 170ms/step\n",
      "Epoch 2/50\n",
      "281/281 - 39s - loss: 0.6920 - accuracy: 0.7727 - val_loss: 5.0048 - val_accuracy: 0.2741 - 39s/epoch - 140ms/step\n",
      "Epoch 3/50\n",
      "281/281 - 39s - loss: 0.4563 - accuracy: 0.8587 - val_loss: 3.1680 - val_accuracy: 0.3882 - 39s/epoch - 140ms/step\n",
      "Epoch 4/50\n",
      "281/281 - 40s - loss: 0.3909 - accuracy: 0.8797 - val_loss: 4.2962 - val_accuracy: 0.4051 - 40s/epoch - 141ms/step\n",
      "Epoch 5/50\n",
      "281/281 - 40s - loss: 0.2613 - accuracy: 0.9204 - val_loss: 7.4182 - val_accuracy: 0.3536 - 40s/epoch - 141ms/step\n",
      "Epoch 6/50\n",
      "281/281 - 40s - loss: 0.3280 - accuracy: 0.8940 - val_loss: 23.7049 - val_accuracy: 0.2563 - 40s/epoch - 142ms/step\n",
      "Epoch 7/50\n",
      "281/281 - 40s - loss: 0.2254 - accuracy: 0.9265 - val_loss: nan - val_accuracy: 0.1506 - 40s/epoch - 142ms/step\n",
      "Epoch 8/50\n",
      "281/281 - 39s - loss: 0.2215 - accuracy: 0.9254 - val_loss: nan - val_accuracy: 0.1029 - 39s/epoch - 140ms/step\n",
      "Epoch 9/50\n",
      "281/281 - 44s - loss: 0.2296 - accuracy: 0.9222 - val_loss: 1031.6226 - val_accuracy: 0.1254 - 44s/epoch - 158ms/step\n",
      "Epoch 10/50\n",
      "281/281 - 42s - loss: 0.2339 - accuracy: 0.9218 - val_loss: 21.9759 - val_accuracy: 0.2844 - 42s/epoch - 148ms/step\n",
      "Epoch 11/50\n",
      "281/281 - 41s - loss: 0.1900 - accuracy: 0.9408 - val_loss: 12.8032 - val_accuracy: 0.2713 - 41s/epoch - 145ms/step\n",
      "Epoch 12/50\n",
      "281/281 - 40s - loss: 0.1509 - accuracy: 0.9529 - val_loss: 4.1217 - val_accuracy: 0.5912 - 40s/epoch - 142ms/step\n",
      "Epoch 13/50\n",
      "281/281 - 40s - loss: 0.1487 - accuracy: 0.9486 - val_loss: 12.3575 - val_accuracy: 0.3134 - 40s/epoch - 142ms/step\n",
      "Epoch 14/50\n",
      "281/281 - 75s - loss: 0.2276 - accuracy: 0.9226 - val_loss: nan - val_accuracy: 0.0973 - 75s/epoch - 267ms/step\n",
      "Epoch 15/50\n",
      "281/281 - 87s - loss: 0.1960 - accuracy: 0.9358 - val_loss: 35.4832 - val_accuracy: 0.3573 - 87s/epoch - 308ms/step\n",
      "Epoch 16/50\n",
      "281/281 - 83s - loss: 0.1526 - accuracy: 0.9472 - val_loss: 20.8335 - val_accuracy: 0.2404 - 83s/epoch - 297ms/step\n",
      "Epoch 17/50\n",
      "281/281 - 87s - loss: 0.1295 - accuracy: 0.9547 - val_loss: 32.2611 - val_accuracy: 0.3751 - 87s/epoch - 309ms/step\n",
      "Epoch 18/50\n",
      "281/281 - 87s - loss: 0.1581 - accuracy: 0.9490 - val_loss: 8.4350 - val_accuracy: 0.4939 - 87s/epoch - 309ms/step\n",
      "Epoch 19/50\n",
      "281/281 - 86s - loss: 0.1309 - accuracy: 0.9532 - val_loss: 38.4677 - val_accuracy: 0.3807 - 86s/epoch - 305ms/step\n",
      "Epoch 20/50\n",
      "281/281 - 87s - loss: 0.1424 - accuracy: 0.9504 - val_loss: 7.7785 - val_accuracy: 0.4986 - 87s/epoch - 309ms/step\n",
      "Epoch 21/50\n",
      "281/281 - 86s - loss: 0.1649 - accuracy: 0.9468 - val_loss: nan - val_accuracy: 0.1141 - 86s/epoch - 305ms/step\n",
      "Epoch 22/50\n",
      "281/281 - 87s - loss: 0.1890 - accuracy: 0.9386 - val_loss: 209.4326 - val_accuracy: 0.0879 - 87s/epoch - 308ms/step\n",
      "Epoch 23/50\n",
      "281/281 - 88s - loss: 0.0897 - accuracy: 0.9693 - val_loss: nan - val_accuracy: 0.3433 - 88s/epoch - 312ms/step\n",
      "Epoch 24/50\n",
      "281/281 - 87s - loss: 0.0620 - accuracy: 0.9775 - val_loss: 2.7249 - val_accuracy: 0.7381 - 87s/epoch - 308ms/step\n",
      "Epoch 25/50\n",
      "281/281 - 88s - loss: 0.0583 - accuracy: 0.9761 - val_loss: 0.8551 - val_accuracy: 0.8335 - 88s/epoch - 312ms/step\n",
      "Epoch 26/50\n",
      "281/281 - 88s - loss: 0.1243 - accuracy: 0.9575 - val_loss: 1.6973 - val_accuracy: 0.7820 - 88s/epoch - 312ms/step\n",
      "Epoch 27/50\n",
      "281/281 - 88s - loss: 0.1704 - accuracy: 0.9418 - val_loss: 56.3155 - val_accuracy: 0.3854 - 88s/epoch - 313ms/step\n",
      "Epoch 28/50\n",
      "281/281 - 88s - loss: 0.2034 - accuracy: 0.9375 - val_loss: 80.3143 - val_accuracy: 0.2554 - 88s/epoch - 315ms/step\n",
      "Epoch 29/50\n",
      "281/281 - 87s - loss: 0.1173 - accuracy: 0.9575 - val_loss: 45.1387 - val_accuracy: 0.3835 - 87s/epoch - 310ms/step\n",
      "Epoch 30/50\n",
      "281/281 - 88s - loss: 0.0931 - accuracy: 0.9714 - val_loss: 94.5975 - val_accuracy: 0.3527 - 88s/epoch - 311ms/step\n",
      "Epoch 31/50\n",
      "281/281 - 88s - loss: 0.0581 - accuracy: 0.9811 - val_loss: 3.6106 - val_accuracy: 0.7727 - 88s/epoch - 312ms/step\n",
      "Epoch 32/50\n",
      "281/281 - 87s - loss: 0.0681 - accuracy: 0.9761 - val_loss: 22.9989 - val_accuracy: 0.5949 - 87s/epoch - 309ms/step\n",
      "Epoch 33/50\n",
      "281/281 - 89s - loss: 0.0806 - accuracy: 0.9714 - val_loss: 2.2418 - val_accuracy: 0.7699 - 89s/epoch - 316ms/step\n",
      "Epoch 34/50\n",
      "281/281 - 87s - loss: 0.1924 - accuracy: 0.9408 - val_loss: 80.5436 - val_accuracy: 0.3218 - 87s/epoch - 311ms/step\n",
      "Epoch 35/50\n",
      "281/281 - 89s - loss: 0.1693 - accuracy: 0.9393 - val_loss: 51.6721 - val_accuracy: 0.5126 - 89s/epoch - 316ms/step\n",
      "Epoch 36/50\n",
      "281/281 - 89s - loss: 0.0632 - accuracy: 0.9761 - val_loss: 8.4013 - val_accuracy: 0.8176 - 89s/epoch - 315ms/step\n",
      "Epoch 37/50\n",
      "281/281 - 87s - loss: 0.0490 - accuracy: 0.9779 - val_loss: 0.8909 - val_accuracy: 0.8962 - 87s/epoch - 311ms/step\n",
      "Epoch 38/50\n",
      "281/281 - 88s - loss: 0.0554 - accuracy: 0.9793 - val_loss: 1.1853 - val_accuracy: 0.8457 - 88s/epoch - 314ms/step\n",
      "Epoch 39/50\n",
      "281/281 - 86s - loss: 0.0431 - accuracy: 0.9836 - val_loss: 1.9093 - val_accuracy: 0.9149 - 86s/epoch - 308ms/step\n",
      "Epoch 40/50\n",
      "281/281 - 86s - loss: 0.1065 - accuracy: 0.9647 - val_loss: 8.4068 - val_accuracy: 0.6782 - 86s/epoch - 306ms/step\n",
      "Epoch 41/50\n",
      "281/281 - 86s - loss: 0.2104 - accuracy: 0.9400 - val_loss: nan - val_accuracy: 0.3312 - 86s/epoch - 308ms/step\n",
      "Epoch 42/50\n",
      "281/281 - 85s - loss: 0.0909 - accuracy: 0.9697 - val_loss: 14.8891 - val_accuracy: 0.5126 - 85s/epoch - 303ms/step\n",
      "Epoch 43/50\n",
      "281/281 - 86s - loss: 0.0627 - accuracy: 0.9789 - val_loss: 4.9376 - val_accuracy: 0.6202 - 86s/epoch - 307ms/step\n",
      "Epoch 44/50\n",
      "281/281 - 86s - loss: 0.0566 - accuracy: 0.9754 - val_loss: 2.4335 - val_accuracy: 0.8148 - 86s/epoch - 305ms/step\n",
      "Epoch 45/50\n",
      "281/281 - 87s - loss: 0.0490 - accuracy: 0.9800 - val_loss: 0.9457 - val_accuracy: 0.8784 - 87s/epoch - 308ms/step\n",
      "Epoch 46/50\n",
      "281/281 - 86s - loss: 0.0766 - accuracy: 0.9729 - val_loss: 1.0637 - val_accuracy: 0.8718 - 86s/epoch - 306ms/step\n",
      "Epoch 47/50\n",
      "281/281 - 85s - loss: 0.1174 - accuracy: 0.9561 - val_loss: 10.6747 - val_accuracy: 0.5276 - 85s/epoch - 303ms/step\n",
      "Epoch 48/50\n",
      "281/281 - 87s - loss: 0.0858 - accuracy: 0.9636 - val_loss: 1.9044 - val_accuracy: 0.7558 - 87s/epoch - 308ms/step\n",
      "Epoch 49/50\n",
      "281/281 - 86s - loss: 0.1460 - accuracy: 0.9504 - val_loss: 5.3102 - val_accuracy: 0.7848 - 86s/epoch - 307ms/step\n",
      "Epoch 50/50\n",
      "281/281 - 86s - loss: 0.0574 - accuracy: 0.9772 - val_loss: 1.2754 - val_accuracy: 0.8531 - 86s/epoch - 305ms/step\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Training MobileNetV3Large...\")\n",
    "mobilenetv3_model = build_model(MobileNetV3Large, input_shape, num_classes)\n",
    "start_time = time.time()\n",
    "history = mobilenetv3_model.fit(\n",
    "    train_batches,\n",
    "    validation_data=valid_batches,\n",
    "    epochs=epochs,\n",
    "    verbose=2,\n",
    ")\n",
    "training_time = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e8fea1f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 912ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Mansal\\anaconda3\\envs\\Mansal\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:1033: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Mansal\\anaconda3\\envs\\Mansal\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:1033: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Mansal\\anaconda3\\envs\\Mansal\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:1033: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Mansal\\anaconda3\\envs\\Mansal\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:1033: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Mansal\\anaconda3\\envs\\Mansal\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:1033: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "val_imgs, val_labels = next(valid_batches)\n",
    "val_preds = mobilenetv3_model.predict(val_imgs)\n",
    "acc = np.mean(np.argmax(val_preds, axis=1) == np.argmax(val_labels, axis=1))\n",
    "mAP = mean_average_precision(val_labels, val_preds)\n",
    "results['MobileNetV3Large'] = {\n",
    "    'training_time_sec': training_time,\n",
    "    'val_accuracy': acc,\n",
    "    'val_mAP': mAP,\n",
    "    'history': history,\n",
    "    'val_imgs': val_imgs,\n",
    "    'val_labels': val_labels,\n",
    "    'val_preds': val_preds\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8eff8a6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 62). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://4581a71a-79de-4a53-b21a-3f650cf1003b/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://4581a71a-79de-4a53-b21a-3f650cf1003b/assets\n"
     ]
    }
   ],
   "source": [
    "with open('mobilenetv3_results.pkl', 'wb') as file:\n",
    "    pickle.dump(results, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7d3753b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: MobileNetV3Large\n",
      "  Training Time: 3732.79 seconds\n",
      "  Validation Accuracy: 0.7000\n",
      "  Validation mAP: 0.4250\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for model_name, res in results.items():\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"  Training Time: {res['training_time_sec']:.2f} seconds\")\n",
    "    print(f\"  Validation Accuracy: {res['val_accuracy']:.4f}\")\n",
    "    print(f\"  Validation mAP: {res['val_mAP']:.4f}\")\n",
    "    print(\"-\" * 40)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Mansal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
